# Native Kubernetes Job for distributed training
#
# For simpler setups without Kubeflow Training Operator.
# Uses indexed Jobs with a headless service.

apiVersion: v1
kind: Service
metadata:
  name: pytorch-workers
  namespace: training
spec:
  clusterIP: None  # Headless service
  selector:
    app: pytorch-training
  ports:
    - port: 29500
      name: nccl
---
apiVersion: batch/v1
kind: Job
metadata:
  name: pytorch-distributed
  namespace: training
spec:
  completions: 4  # Number of workers
  parallelism: 4  # Run all at once
  completionMode: Indexed
  template:
    metadata:
      labels:
        app: pytorch-training
    spec:
      restartPolicy: OnFailure
      subdomain: pytorch-workers  # For DNS resolution

      tolerations:
        - key: nvidia.com/gpu
          operator: Equal
          value: "true"
          effect: NoSchedule

      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: nvidia.com/gpu
                    operator: In
                    values:
                      - "true"

      initContainers:
        # Wait for all workers to be ready
        - name: wait-for-workers
          image: busybox:1.36
          command:
            - sh
            - -c
            - |
              echo "Waiting for all workers..."
              for i in $(seq 0 3); do
                until nslookup pytorch-distributed-${i}.pytorch-workers.training.svc.cluster.local; do
                  echo "Waiting for worker ${i}..."
                  sleep 2
                done
              done
              echo "All workers ready"

      containers:
        - name: pytorch
          image: pytorch/pytorch:2.2.0-cuda12.1-cudnn8-runtime
          command:
            - sh
            - -c
            - |
              # Set rank from Job index
              export RANK=$JOB_COMPLETION_INDEX
              export LOCAL_RANK=0
              export WORLD_SIZE=4
              export MASTER_ADDR=pytorch-distributed-0.pytorch-workers.training.svc.cluster.local
              export MASTER_PORT=29500

              python -m torch.distributed.run \
                --nnodes=$WORLD_SIZE \
                --node_rank=$RANK \
                --nproc_per_node=1 \
                --master_addr=$MASTER_ADDR \
                --master_port=$MASTER_PORT \
                /scripts/train_ddp.py \
                --epochs=10 \
                --checkpoint-dir=/checkpoints
          env:
            - name: NCCL_DEBUG
              value: "WARN"
          resources:
            requests:
              cpu: "4"
              memory: "32Gi"
              nvidia.com/gpu: 1
            limits:
              cpu: "8"
              memory: "64Gi"
              nvidia.com/gpu: 1
          volumeMounts:
            - name: scripts
              mountPath: /scripts
            - name: checkpoints
              mountPath: /checkpoints
            - name: dshm
              mountPath: /dev/shm

      volumes:
        - name: scripts
          configMap:
            name: training-scripts
        - name: checkpoints
          persistentVolumeClaim:
            claimName: training-checkpoints
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
