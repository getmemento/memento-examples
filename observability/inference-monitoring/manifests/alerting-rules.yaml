# Alerting Rules for Inference Workloads

apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: inference-alerts
  namespace: monitoring
  labels:
    app: prometheus
spec:
  groups:
    - name: inference-latency
      rules:
        - alert: InferenceHighLatency
          expr: inference:latency:p95 > 10
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "P95 inference latency is {{ $value | humanizeDuration }}"
            description: "Inference latency has exceeded 10 seconds for 5 minutes"
            runbook_url: "https://docs.example.com/runbooks/high-inference-latency"

        - alert: InferenceCriticalLatency
          expr: inference:latency:p95 > 30
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "P95 inference latency critical at {{ $value | humanizeDuration }}"
            description: "Users are experiencing very high latency"

        - alert: InferenceHighTTFT
          expr: inference:ttft:p95 > 2
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "P95 TTFT is {{ $value | humanizeDuration }}"
            description: "Time to first token is high, affecting user experience"

    - name: inference-queue
      rules:
        - alert: InferenceQueueBuildingUp
          expr: inference:queue_depth > 20
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "{{ $value }} requests waiting in inference queue"
            description: "Queue is building up, consider scaling"

        - alert: InferenceQueueCritical
          expr: inference:queue_depth > 50
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "Critical queue depth: {{ $value }} requests"
            description: "Queue is critically high, immediate action required"

        - alert: InferenceQueueGrowing
          expr: |
            deriv(inference:queue_depth[10m]) > 0.5
            and inference:queue_depth > 10
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Inference queue is growing"
            description: "Queue depth increasing, may need to scale"

    - name: inference-efficiency
      rules:
        - alert: InferenceHighKVCacheUsage
          expr: inference:kv_cache_usage > 95
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "KV cache usage at {{ $value }}%"
            description: "High cache usage may cause preemptions"

        - alert: InferenceFrequentPreemptions
          expr: inference:preemption_rate > 0.1
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High preemption rate: {{ $value }}/s"
            description: "Memory pressure causing request preemptions"

        - alert: InferenceLowThroughput
          expr: |
            inference:tokens_per_second < 50
            and inference:running_requests > 0
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Token throughput is {{ $value }} tok/s"
            description: "Throughput is lower than expected"

    - name: inference-availability
      rules:
        - alert: InferenceNoPodsReady
          expr: |
            kube_deployment_status_replicas_available{deployment="vllm"} == 0
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "No inference pods available"
            description: "All inference pods are down"

        - alert: InferencePodsNotReady
          expr: |
            kube_deployment_status_replicas_available{deployment="vllm"}
            < kube_deployment_spec_replicas{deployment="vllm"}
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Some inference pods not ready"
            description: "{{ $value }} replicas available, expected more"

    - name: inference-slo
      rules:
        - alert: InferenceSLOLatencyBreach
          expr: inference:slo:latency_5s_ratio < 0.95
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "Latency SLO breach: {{ $value | humanizePercentage }} within 5s"
            description: "Less than 95% of requests completing within 5s"

        - alert: InferenceSLOTTFTBreach
          expr: inference:slo:ttft_1s_ratio < 0.99
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "TTFT SLO breach: {{ $value | humanizePercentage }} within 1s"
            description: "Less than 99% of requests getting first token within 1s"
